{"title":"训练深度学习网络的挑战和解决方法","uid":"a71e8b888458c8851815d0b96809bb00","slug":"训练深度学习网络的挑战和解决方法","date":"2019-12-27T09:46:48.000Z","updated":"2021-08-19T08:49:11.231Z","comments":true,"path":"api/articles/训练深度学习网络的挑战和解决方法.json","keywords":null,"cover":"https://image-assets.mihuashi.com/2021/08/15/09/FixrUuTczBtvQo8U9j-X2eCGMrzm.png","content":"<p>Hands-on ml2 Ch.11 读书笔记。</p>\n<hr>\n<h2 id=\"Vanishing-Exploding-Gradients\"><a href=\"#Vanishing-Exploding-Gradients\" class=\"headerlink\" title=\"Vanishing/Exploding Gradients\"></a>Vanishing/Exploding Gradients</h2><p>在深度学习网络训练过程中，更深层次的网络梯度下降幅度越小，导致深层次的网络权重保持不变。或一些层梯度过大，每次权值更新的幅度很大（如 RNN）。导致不同层的学习速度相差较大。</p>\n<p>这个问题的出现可归功于 Sigmoid 激活函数和正态分布权值初始化方法的使用。由于 Sigmoid 激活函数在正无穷和负无穷处收敛，其导数趋向于 0。</p>\n<h2 id=\"Glorot-and-He-Initialization\"><a href=\"#Glorot-and-He-Initialization\" class=\"headerlink\" title=\"Glorot and He Initialization\"></a>Glorot and He Initialization</h2><p>因此，建议在使用 Sigmoid 激活函数时采用 Glorot 初始化权值，或使用 ReLU 激活函数时采用 He 初始化权值，以及使用 SELU 激活函数时采用 LeCun 初始化权值。</p>\n<p>Keras 默认使用正态分布的 Glorot 初始化权值。</p>\n<h2 id=\"不收敛的激活函数\"><a href=\"#不收敛的激活函数\" class=\"headerlink\" title=\"不收敛的激活函数\"></a>不收敛的激活函数</h2><p>在深度学习网络中使用 ReLU 激活函数的效果比 Sigmoid 激活函数好很多。但会出现 <em>dying ReLUs</em> 的问题：一些神经元的激活值一直为0。</p>\n<p>可以使用 <em>leaky ReLU</em> 函数解决这个问题 <mjx-container class=\"MathJax\" jax=\"SVG\"><svg style=\"vertical-align: -0.566ex\" xmlns=\"http://www.w3.org/2000/svg\" width=\"29.773ex\" height=\"2.262ex\" role=\"img\" focusable=\"false\" viewBox=\"0 -750 13159.8 1000\"><g stroke=\"currentColor\" fill=\"currentColor\" stroke-width=\"0\" transform=\"scale(1,-1)\"><g data-mml-node=\"math\"><g data-mml-node=\"mi\"><path data-c=\"1D43F\" d=\"M228 637Q194 637 192 641Q191 643 191 649Q191 673 202 682Q204 683 217 683Q271 680 344 680Q485 680 506 683H518Q524 677 524 674T522 656Q517 641 513 637H475Q406 636 394 628Q387 624 380 600T313 336Q297 271 279 198T252 88L243 52Q243 48 252 48T311 46H328Q360 46 379 47T428 54T478 72T522 106T564 161Q580 191 594 228T611 270Q616 273 628 273H641Q647 264 647 262T627 203T583 83T557 9Q555 4 553 3T537 0T494 -1Q483 -1 418 -1T294 0H116Q32 0 32 10Q32 17 34 24Q39 43 44 45Q48 46 59 46H65Q92 46 125 49Q139 52 144 61Q147 65 216 339T285 628Q285 635 228 637Z\"></path></g><g data-mml-node=\"mi\" transform=\"translate(681,0)\"><path data-c=\"1D452\" d=\"M39 168Q39 225 58 272T107 350T174 402T244 433T307 442H310Q355 442 388 420T421 355Q421 265 310 237Q261 224 176 223Q139 223 138 221Q138 219 132 186T125 128Q125 81 146 54T209 26T302 45T394 111Q403 121 406 121Q410 121 419 112T429 98T420 82T390 55T344 24T281 -1T205 -11Q126 -11 83 42T39 168ZM373 353Q367 405 305 405Q272 405 244 391T199 357T170 316T154 280T149 261Q149 260 169 260Q282 260 327 284T373 353Z\"></path></g><g data-mml-node=\"mi\" transform=\"translate(1147,0)\"><path data-c=\"1D44E\" d=\"M33 157Q33 258 109 349T280 441Q331 441 370 392Q386 422 416 422Q429 422 439 414T449 394Q449 381 412 234T374 68Q374 43 381 35T402 26Q411 27 422 35Q443 55 463 131Q469 151 473 152Q475 153 483 153H487Q506 153 506 144Q506 138 501 117T481 63T449 13Q436 0 417 -8Q409 -10 393 -10Q359 -10 336 5T306 36L300 51Q299 52 296 50Q294 48 292 46Q233 -10 172 -10Q117 -10 75 30T33 157ZM351 328Q351 334 346 350T323 385T277 405Q242 405 210 374T160 293Q131 214 119 129Q119 126 119 118T118 106Q118 61 136 44T179 26Q217 26 254 59T298 110Q300 114 325 217T351 328Z\"></path></g><g data-mml-node=\"mi\" transform=\"translate(1676,0)\"><path data-c=\"1D458\" d=\"M121 647Q121 657 125 670T137 683Q138 683 209 688T282 694Q294 694 294 686Q294 679 244 477Q194 279 194 272Q213 282 223 291Q247 309 292 354T362 415Q402 442 438 442Q468 442 485 423T503 369Q503 344 496 327T477 302T456 291T438 288Q418 288 406 299T394 328Q394 353 410 369T442 390L458 393Q446 405 434 405H430Q398 402 367 380T294 316T228 255Q230 254 243 252T267 246T293 238T320 224T342 206T359 180T365 147Q365 130 360 106T354 66Q354 26 381 26Q429 26 459 145Q461 153 479 153H483Q499 153 499 144Q499 139 496 130Q455 -11 378 -11Q333 -11 305 15T277 90Q277 108 280 121T283 145Q283 167 269 183T234 206T200 217T182 220H180Q168 178 159 139T145 81T136 44T129 20T122 7T111 -2Q98 -11 83 -11Q66 -11 57 -1T48 16Q48 26 85 176T158 471L195 616Q196 629 188 632T149 637H144Q134 637 131 637T124 640T121 647Z\"></path></g><g data-mml-node=\"mi\" transform=\"translate(2197,0)\"><path data-c=\"1D466\" d=\"M21 287Q21 301 36 335T84 406T158 442Q199 442 224 419T250 355Q248 336 247 334Q247 331 231 288T198 191T182 105Q182 62 196 45T238 27Q261 27 281 38T312 61T339 94Q339 95 344 114T358 173T377 247Q415 397 419 404Q432 431 462 431Q475 431 483 424T494 412T496 403Q496 390 447 193T391 -23Q363 -106 294 -155T156 -205Q111 -205 77 -183T43 -117Q43 -95 50 -80T69 -58T89 -48T106 -45Q150 -45 150 -87Q150 -107 138 -122T115 -142T102 -147L99 -148Q101 -153 118 -160T152 -167H160Q177 -167 186 -165Q219 -156 247 -127T290 -65T313 -9T321 21L315 17Q309 13 296 6T270 -6Q250 -11 231 -11Q185 -11 150 11T104 82Q103 89 103 113Q103 170 138 262T173 379Q173 380 173 381Q173 390 173 393T169 400T158 404H154Q131 404 112 385T82 344T65 302T57 280Q55 278 41 278H27Q21 284 21 287Z\"></path></g><g data-mml-node=\"mi\" transform=\"translate(2687,0)\"><path data-c=\"1D445\" d=\"M230 637Q203 637 198 638T193 649Q193 676 204 682Q206 683 378 683Q550 682 564 680Q620 672 658 652T712 606T733 563T739 529Q739 484 710 445T643 385T576 351T538 338L545 333Q612 295 612 223Q612 212 607 162T602 80V71Q602 53 603 43T614 25T640 16Q668 16 686 38T712 85Q717 99 720 102T735 105Q755 105 755 93Q755 75 731 36Q693 -21 641 -21H632Q571 -21 531 4T487 82Q487 109 502 166T517 239Q517 290 474 313Q459 320 449 321T378 323H309L277 193Q244 61 244 59Q244 55 245 54T252 50T269 48T302 46H333Q339 38 339 37T336 19Q332 6 326 0H311Q275 2 180 2Q146 2 117 2T71 2T50 1Q33 1 33 10Q33 12 36 24Q41 43 46 45Q50 46 61 46H67Q94 46 127 49Q141 52 146 61Q149 65 218 339T287 628Q287 635 230 637ZM630 554Q630 586 609 608T523 636Q521 636 500 636T462 637H440Q393 637 386 627Q385 624 352 494T319 361Q319 360 388 360Q466 361 492 367Q556 377 592 426Q608 449 619 486T630 554Z\"></path></g><g data-mml-node=\"mi\" transform=\"translate(3446,0)\"><path data-c=\"1D452\" d=\"M39 168Q39 225 58 272T107 350T174 402T244 433T307 442H310Q355 442 388 420T421 355Q421 265 310 237Q261 224 176 223Q139 223 138 221Q138 219 132 186T125 128Q125 81 146 54T209 26T302 45T394 111Q403 121 406 121Q410 121 419 112T429 98T420 82T390 55T344 24T281 -1T205 -11Q126 -11 83 42T39 168ZM373 353Q367 405 305 405Q272 405 244 391T199 357T170 316T154 280T149 261Q149 260 169 260Q282 260 327 284T373 353Z\"></path></g><g data-mml-node=\"mi\" transform=\"translate(3912,0)\"><path data-c=\"1D43F\" d=\"M228 637Q194 637 192 641Q191 643 191 649Q191 673 202 682Q204 683 217 683Q271 680 344 680Q485 680 506 683H518Q524 677 524 674T522 656Q517 641 513 637H475Q406 636 394 628Q387 624 380 600T313 336Q297 271 279 198T252 88L243 52Q243 48 252 48T311 46H328Q360 46 379 47T428 54T478 72T522 106T564 161Q580 191 594 228T611 270Q616 273 628 273H641Q647 264 647 262T627 203T583 83T557 9Q555 4 553 3T537 0T494 -1Q483 -1 418 -1T294 0H116Q32 0 32 10Q32 17 34 24Q39 43 44 45Q48 46 59 46H65Q92 46 125 49Q139 52 144 61Q147 65 216 339T285 628Q285 635 228 637Z\"></path></g><g data-mml-node=\"msub\" transform=\"translate(4593,0)\"><g data-mml-node=\"mi\"><path data-c=\"1D448\" d=\"M107 637Q73 637 71 641Q70 643 70 649Q70 673 81 682Q83 683 98 683Q139 681 234 681Q268 681 297 681T342 682T362 682Q378 682 378 672Q378 670 376 658Q371 641 366 638H364Q362 638 359 638T352 638T343 637T334 637Q295 636 284 634T266 623Q265 621 238 518T184 302T154 169Q152 155 152 140Q152 86 183 55T269 24Q336 24 403 69T501 205L552 406Q599 598 599 606Q599 633 535 637Q511 637 511 648Q511 650 513 660Q517 676 519 679T529 683Q532 683 561 682T645 680Q696 680 723 681T752 682Q767 682 767 672Q767 650 759 642Q756 637 737 637Q666 633 648 597Q646 592 598 404Q557 235 548 205Q515 105 433 42T263 -22Q171 -22 116 34T60 167V183Q60 201 115 421Q164 622 164 628Q164 635 107 637Z\"></path></g><g data-mml-node=\"TeXAtom\" transform=\"translate(716,-150) scale(0.707)\" data-mjx-texclass=\"ORD\"><g data-mml-node=\"mi\"><path data-c=\"1D6FC\" d=\"M34 156Q34 270 120 356T309 442Q379 442 421 402T478 304Q484 275 485 237V208Q534 282 560 374Q564 388 566 390T582 393Q603 393 603 385Q603 376 594 346T558 261T497 161L486 147L487 123Q489 67 495 47T514 26Q528 28 540 37T557 60Q559 67 562 68T577 70Q597 70 597 62Q597 56 591 43Q579 19 556 5T512 -10H505Q438 -10 414 62L411 69L400 61Q390 53 370 41T325 18T267 -2T203 -11Q124 -11 79 39T34 156ZM208 26Q257 26 306 47T379 90L403 112Q401 255 396 290Q382 405 304 405Q235 405 183 332Q156 292 139 224T121 120Q121 71 146 49T208 26Z\"></path></g></g></g><g data-mml-node=\"mo\" transform=\"translate(5811.5,0)\"><path data-c=\"28\" d=\"M94 250Q94 319 104 381T127 488T164 576T202 643T244 695T277 729T302 750H315H319Q333 750 333 741Q333 738 316 720T275 667T226 581T184 443T167 250T184 58T225 -81T274 -167T316 -220T333 -241Q333 -250 318 -250H315H302L274 -226Q180 -141 137 -14T94 250Z\"></path></g><g data-mml-node=\"mi\" transform=\"translate(6200.5,0)\"><path data-c=\"1D467\" d=\"M347 338Q337 338 294 349T231 360Q211 360 197 356T174 346T162 335T155 324L153 320Q150 317 138 317Q117 317 117 325Q117 330 120 339Q133 378 163 406T229 440Q241 442 246 442Q271 442 291 425T329 392T367 375Q389 375 411 408T434 441Q435 442 449 442H462Q468 436 468 434Q468 430 463 420T449 399T432 377T418 358L411 349Q368 298 275 214T160 106L148 94L163 93Q185 93 227 82T290 71Q328 71 360 90T402 140Q406 149 409 151T424 153Q443 153 443 143Q443 138 442 134Q425 72 376 31T278 -11Q252 -11 232 6T193 40T155 57Q111 57 76 -3Q70 -11 59 -11H54H41Q35 -5 35 -2Q35 13 93 84Q132 129 225 214T340 322Q352 338 347 338Z\"></path></g><g data-mml-node=\"mo\" transform=\"translate(6665.5,0)\"><path data-c=\"29\" d=\"M60 749L64 750Q69 750 74 750H86L114 726Q208 641 251 514T294 250Q294 182 284 119T261 12T224 -76T186 -143T145 -194T113 -227T90 -246Q87 -249 86 -250H74Q66 -250 63 -250T58 -247T55 -238Q56 -237 66 -225Q221 -64 221 250T66 725Q56 737 55 738Q55 746 60 749Z\"></path></g><g data-mml-node=\"mo\" transform=\"translate(7332.3,0)\"><path data-c=\"3D\" d=\"M56 347Q56 360 70 367H707Q722 359 722 347Q722 336 708 328L390 327H72Q56 332 56 347ZM56 153Q56 168 72 173H708Q722 163 722 153Q722 140 707 133H70Q56 140 56 153Z\"></path></g><g data-mml-node=\"mi\" transform=\"translate(8388.1,0)\"><path data-c=\"1D45A\" d=\"M21 287Q22 293 24 303T36 341T56 388T88 425T132 442T175 435T205 417T221 395T229 376L231 369Q231 367 232 367L243 378Q303 442 384 442Q401 442 415 440T441 433T460 423T475 411T485 398T493 385T497 373T500 364T502 357L510 367Q573 442 659 442Q713 442 746 415T780 336Q780 285 742 178T704 50Q705 36 709 31T724 26Q752 26 776 56T815 138Q818 149 821 151T837 153Q857 153 857 145Q857 144 853 130Q845 101 831 73T785 17T716 -10Q669 -10 648 17T627 73Q627 92 663 193T700 345Q700 404 656 404H651Q565 404 506 303L499 291L466 157Q433 26 428 16Q415 -11 385 -11Q372 -11 364 -4T353 8T350 18Q350 29 384 161L420 307Q423 322 423 345Q423 404 379 404H374Q288 404 229 303L222 291L189 157Q156 26 151 16Q138 -11 108 -11Q95 -11 87 -5T76 7T74 17Q74 30 112 181Q151 335 151 342Q154 357 154 369Q154 405 129 405Q107 405 92 377T69 316T57 280Q55 278 41 278H27Q21 284 21 287Z\"></path></g><g data-mml-node=\"mi\" transform=\"translate(9266.1,0)\"><path data-c=\"1D44E\" d=\"M33 157Q33 258 109 349T280 441Q331 441 370 392Q386 422 416 422Q429 422 439 414T449 394Q449 381 412 234T374 68Q374 43 381 35T402 26Q411 27 422 35Q443 55 463 131Q469 151 473 152Q475 153 483 153H487Q506 153 506 144Q506 138 501 117T481 63T449 13Q436 0 417 -8Q409 -10 393 -10Q359 -10 336 5T306 36L300 51Q299 52 296 50Q294 48 292 46Q233 -10 172 -10Q117 -10 75 30T33 157ZM351 328Q351 334 346 350T323 385T277 405Q242 405 210 374T160 293Q131 214 119 129Q119 126 119 118T118 106Q118 61 136 44T179 26Q217 26 254 59T298 110Q300 114 325 217T351 328Z\"></path></g><g data-mml-node=\"mi\" transform=\"translate(9795.1,0)\"><path data-c=\"1D465\" d=\"M52 289Q59 331 106 386T222 442Q257 442 286 424T329 379Q371 442 430 442Q467 442 494 420T522 361Q522 332 508 314T481 292T458 288Q439 288 427 299T415 328Q415 374 465 391Q454 404 425 404Q412 404 406 402Q368 386 350 336Q290 115 290 78Q290 50 306 38T341 26Q378 26 414 59T463 140Q466 150 469 151T485 153H489Q504 153 504 145Q504 144 502 134Q486 77 440 33T333 -11Q263 -11 227 52Q186 -10 133 -10H127Q78 -10 57 16T35 71Q35 103 54 123T99 143Q142 143 142 101Q142 81 130 66T107 46T94 41L91 40Q91 39 97 36T113 29T132 26Q168 26 194 71Q203 87 217 139T245 247T261 313Q266 340 266 352Q266 380 251 392T217 404Q177 404 142 372T93 290Q91 281 88 280T72 278H58Q52 284 52 289Z\"></path></g><g data-mml-node=\"mo\" transform=\"translate(10367.1,0)\"><path data-c=\"28\" d=\"M94 250Q94 319 104 381T127 488T164 576T202 643T244 695T277 729T302 750H315H319Q333 750 333 741Q333 738 316 720T275 667T226 581T184 443T167 250T184 58T225 -81T274 -167T316 -220T333 -241Q333 -250 318 -250H315H302L274 -226Q180 -141 137 -14T94 250Z\"></path></g><g data-mml-node=\"mi\" transform=\"translate(10756.1,0)\"><path data-c=\"1D6FC\" d=\"M34 156Q34 270 120 356T309 442Q379 442 421 402T478 304Q484 275 485 237V208Q534 282 560 374Q564 388 566 390T582 393Q603 393 603 385Q603 376 594 346T558 261T497 161L486 147L487 123Q489 67 495 47T514 26Q528 28 540 37T557 60Q559 67 562 68T577 70Q597 70 597 62Q597 56 591 43Q579 19 556 5T512 -10H505Q438 -10 414 62L411 69L400 61Q390 53 370 41T325 18T267 -2T203 -11Q124 -11 79 39T34 156ZM208 26Q257 26 306 47T379 90L403 112Q401 255 396 290Q382 405 304 405Q235 405 183 332Q156 292 139 224T121 120Q121 71 146 49T208 26Z\"></path></g><g data-mml-node=\"mi\" transform=\"translate(11396.1,0)\"><path data-c=\"1D467\" d=\"M347 338Q337 338 294 349T231 360Q211 360 197 356T174 346T162 335T155 324L153 320Q150 317 138 317Q117 317 117 325Q117 330 120 339Q133 378 163 406T229 440Q241 442 246 442Q271 442 291 425T329 392T367 375Q389 375 411 408T434 441Q435 442 449 442H462Q468 436 468 434Q468 430 463 420T449 399T432 377T418 358L411 349Q368 298 275 214T160 106L148 94L163 93Q185 93 227 82T290 71Q328 71 360 90T402 140Q406 149 409 151T424 153Q443 153 443 143Q443 138 442 134Q425 72 376 31T278 -11Q252 -11 232 6T193 40T155 57Q111 57 76 -3Q70 -11 59 -11H54H41Q35 -5 35 -2Q35 13 93 84Q132 129 225 214T340 322Q352 338 347 338Z\"></path></g><g data-mml-node=\"mo\" transform=\"translate(11861.1,0)\"><path data-c=\"2C\" d=\"M78 35T78 60T94 103T137 121Q165 121 187 96T210 8Q210 -27 201 -60T180 -117T154 -158T130 -185T117 -194Q113 -194 104 -185T95 -172Q95 -168 106 -156T131 -126T157 -76T173 -3V9L172 8Q170 7 167 6T161 3T152 1T140 0Q113 0 96 17Z\"></path></g><g data-mml-node=\"mi\" transform=\"translate(12305.8,0)\"><path data-c=\"1D467\" d=\"M347 338Q337 338 294 349T231 360Q211 360 197 356T174 346T162 335T155 324L153 320Q150 317 138 317Q117 317 117 325Q117 330 120 339Q133 378 163 406T229 440Q241 442 246 442Q271 442 291 425T329 392T367 375Q389 375 411 408T434 441Q435 442 449 442H462Q468 436 468 434Q468 430 463 420T449 399T432 377T418 358L411 349Q368 298 275 214T160 106L148 94L163 93Q185 93 227 82T290 71Q328 71 360 90T402 140Q406 149 409 151T424 153Q443 153 443 143Q443 138 442 134Q425 72 376 31T278 -11Q252 -11 232 6T193 40T155 57Q111 57 76 -3Q70 -11 59 -11H54H41Q35 -5 35 -2Q35 13 93 84Q132 129 225 214T340 322Q352 338 347 338Z\"></path></g><g data-mml-node=\"mo\" transform=\"translate(12770.8,0)\"><path data-c=\"29\" d=\"M60 749L64 750Q69 750 74 750H86L114 726Q208 641 251 514T294 250Q294 182 284 119T261 12T224 -76T186 -143T145 -194T113 -227T90 -246Q87 -249 86 -250H74Q66 -250 63 -250T58 -247T55 -238Q56 -237 66 -225Q221 -64 221 250T66 725Q56 737 55 738Q55 746 60 749Z\"></path></g></g></g></svg></mjx-container>。即引入一个超参数 <mjx-container class=\"MathJax\" jax=\"SVG\"><svg style=\"vertical-align: -0.025ex\" xmlns=\"http://www.w3.org/2000/svg\" width=\"1.448ex\" height=\"1.025ex\" role=\"img\" focusable=\"false\" viewBox=\"0 -442 640 453\"><g stroke=\"currentColor\" fill=\"currentColor\" stroke-width=\"0\" transform=\"scale(1,-1)\"><g data-mml-node=\"math\"><g data-mml-node=\"mi\"><path data-c=\"1D6FC\" d=\"M34 156Q34 270 120 356T309 442Q379 442 421 402T478 304Q484 275 485 237V208Q534 282 560 374Q564 388 566 390T582 393Q603 393 603 385Q603 376 594 346T558 261T497 161L486 147L487 123Q489 67 495 47T514 26Q528 28 540 37T557 60Q559 67 562 68T577 70Q597 70 597 62Q597 56 591 43Q579 19 556 5T512 -10H505Q438 -10 414 62L411 69L400 61Q390 53 370 41T325 18T267 -2T203 -11Q124 -11 79 39T34 156ZM208 26Q257 26 306 47T379 90L403 112Q401 255 396 290Q382 405 304 405Q235 405 183 332Q156 292 139 224T121 120Q121 71 146 49T208 26Z\"></path></g></g></g></svg></mjx-container> 定义当输入值 <mjx-container class=\"MathJax\" jax=\"SVG\"><svg style=\"vertical-align: -0.09ex\" xmlns=\"http://www.w3.org/2000/svg\" width=\"5.2ex\" height=\"1.597ex\" role=\"img\" focusable=\"false\" viewBox=\"0 -666 2298.6 706\"><g stroke=\"currentColor\" fill=\"currentColor\" stroke-width=\"0\" transform=\"scale(1,-1)\"><g data-mml-node=\"math\"><g data-mml-node=\"mi\"><path data-c=\"1D467\" d=\"M347 338Q337 338 294 349T231 360Q211 360 197 356T174 346T162 335T155 324L153 320Q150 317 138 317Q117 317 117 325Q117 330 120 339Q133 378 163 406T229 440Q241 442 246 442Q271 442 291 425T329 392T367 375Q389 375 411 408T434 441Q435 442 449 442H462Q468 436 468 434Q468 430 463 420T449 399T432 377T418 358L411 349Q368 298 275 214T160 106L148 94L163 93Q185 93 227 82T290 71Q328 71 360 90T402 140Q406 149 409 151T424 153Q443 153 443 143Q443 138 442 134Q425 72 376 31T278 -11Q252 -11 232 6T193 40T155 57Q111 57 76 -3Q70 -11 59 -11H54H41Q35 -5 35 -2Q35 13 93 84Q132 129 225 214T340 322Q352 338 347 338Z\"></path></g><g data-mml-node=\"mo\" transform=\"translate(742.8,0)\"><path data-c=\"3C\" d=\"M694 -11T694 -19T688 -33T678 -40Q671 -40 524 29T234 166L90 235Q83 240 83 250Q83 261 91 266Q664 540 678 540Q681 540 687 534T694 519T687 505Q686 504 417 376L151 250L417 124Q686 -4 687 -5Q694 -11 694 -19Z\"></path></g><g data-mml-node=\"mn\" transform=\"translate(1798.6,0)\"><path data-c=\"30\" d=\"M96 585Q152 666 249 666Q297 666 345 640T423 548Q460 465 460 320Q460 165 417 83Q397 41 362 16T301 -15T250 -22Q224 -22 198 -16T137 16T82 83Q39 165 39 320Q39 494 96 585ZM321 597Q291 629 250 629Q208 629 178 597Q153 571 145 525T137 333Q137 175 145 125T181 46Q209 16 250 16Q290 16 318 46Q347 76 354 130T362 333Q362 478 354 524T321 597Z\"></path></g></g></g></svg></mjx-container> 时函数的“泄露量”。<em>Randomized leaky ReLU</em>(RReLU) 是 <em>leaky ReLU</em> 的变种，即在训练过程中随机指定某一范围内的 <mjx-container class=\"MathJax\" jax=\"SVG\"><svg style=\"vertical-align: -0.025ex\" xmlns=\"http://www.w3.org/2000/svg\" width=\"1.448ex\" height=\"1.025ex\" role=\"img\" focusable=\"false\" viewBox=\"0 -442 640 453\"><g stroke=\"currentColor\" fill=\"currentColor\" stroke-width=\"0\" transform=\"scale(1,-1)\"><g data-mml-node=\"math\"><g data-mml-node=\"mi\"><path data-c=\"1D6FC\" d=\"M34 156Q34 270 120 356T309 442Q379 442 421 402T478 304Q484 275 485 237V208Q534 282 560 374Q564 388 566 390T582 393Q603 393 603 385Q603 376 594 346T558 261T497 161L486 147L487 123Q489 67 495 47T514 26Q528 28 540 37T557 60Q559 67 562 68T577 70Q597 70 597 62Q597 56 591 43Q579 19 556 5T512 -10H505Q438 -10 414 62L411 69L400 61Q390 53 370 41T325 18T267 -2T203 -11Q124 -11 79 39T34 156ZM208 26Q257 26 306 47T379 90L403 112Q401 255 396 290Q382 405 304 405Q235 405 183 332Q156 292 139 224T121 120Q121 71 146 49T208 26Z\"></path></g></g></g></svg></mjx-container> 值，测试时则取训练中 <mjx-container class=\"MathJax\" jax=\"SVG\"><svg style=\"vertical-align: -0.025ex\" xmlns=\"http://www.w3.org/2000/svg\" width=\"1.448ex\" height=\"1.025ex\" role=\"img\" focusable=\"false\" viewBox=\"0 -442 640 453\"><g stroke=\"currentColor\" fill=\"currentColor\" stroke-width=\"0\" transform=\"scale(1,-1)\"><g data-mml-node=\"math\"><g data-mml-node=\"mi\"><path data-c=\"1D6FC\" d=\"M34 156Q34 270 120 356T309 442Q379 442 421 402T478 304Q484 275 485 237V208Q534 282 560 374Q564 388 566 390T582 393Q603 393 603 385Q603 376 594 346T558 261T497 161L486 147L487 123Q489 67 495 47T514 26Q528 28 540 37T557 60Q559 67 562 68T577 70Q597 70 597 62Q597 56 591 43Q579 19 556 5T512 -10H505Q438 -10 414 62L411 69L400 61Q390 53 370 41T325 18T267 -2T203 -11Q124 -11 79 39T34 156ZM208 26Q257 26 306 47T379 90L403 112Q401 255 396 290Q382 405 304 405Q235 405 183 332Q156 292 139 224T121 120Q121 71 146 49T208 26Z\"></path></g></g></g></svg></mjx-container> 的平均值。</p>\n<p>2015年，<em>exponential linear unit</em>（ELU）激活函数被提出。后来，有人提出在只由 dense 层构建的神经网络中使用 SELU 函数（S for scale），网络会 <em>self-normalize</em>：训练过程中，每一层的输出会保持平均值为0，标准差为1，即不会出现 Vanishing/Exploding Gradients。但出现这种现象需要一定条件：</p>\n<ul>\n<li>输入层必须是正则的（均值为0，标准差为1）；</li>\n<li>每个隐藏层的权值必须是 LeCun 初始化；</li>\n<li>网络的架构必须是连续的，例如在 RNN 中 SELU 的效果就不如其他激活函数；</li>\n<li>必须是只由 dense 层构建的神经网络，然而实际 SELU 在 CNN 中的效果也很好。</li>\n</ul>\n<p>因此选择激活函数：SELU&gt;ELU&gt;leaky ReLU（及其变种）&gt;ReLU&gt;tanh&gt;logistic(sigmoid)。如果网络不是 self-normalize 的，ELU 的效果会比 SELU 更佳。如果不考虑运行延迟，可以选用 leaky ReLU。不想调参可以用 Keras 使用的 <mjx-container class=\"MathJax\" jax=\"SVG\"><svg style=\"vertical-align: -0.025ex\" xmlns=\"http://www.w3.org/2000/svg\" width=\"1.448ex\" height=\"1.025ex\" role=\"img\" focusable=\"false\" viewBox=\"0 -442 640 453\"><g stroke=\"currentColor\" fill=\"currentColor\" stroke-width=\"0\" transform=\"scale(1,-1)\"><g data-mml-node=\"math\"><g data-mml-node=\"mi\"><path data-c=\"1D6FC\" d=\"M34 156Q34 270 120 356T309 442Q379 442 421 402T478 304Q484 275 485 237V208Q534 282 560 374Q564 388 566 390T582 393Q603 393 603 385Q603 376 594 346T558 261T497 161L486 147L487 123Q489 67 495 47T514 26Q528 28 540 37T557 60Q559 67 562 68T577 70Q597 70 597 62Q597 56 591 43Q579 19 556 5T512 -10H505Q438 -10 414 62L411 69L400 61Q390 53 370 41T325 18T267 -2T203 -11Q124 -11 79 39T34 156ZM208 26Q257 26 306 47T379 90L403 112Q401 255 396 290Q382 405 304 405Q235 405 183 332Q156 292 139 224T121 120Q121 71 146 49T208 26Z\"></path></g></g></g></svg></mjx-container> 值（0.3）。算力允许的话，也可以使用交叉验证寻找其他激活函数，比如过拟合时考虑用 RReLU，训练集较大时用 PReLU。</p>\n<h2 id=\"Batch-Normalization\"><a href=\"#Batch-Normalization\" class=\"headerlink\" title=\"Batch Normalization\"></a>Batch Normalization</h2><p><em>Batch normalization</em> 可以降低网络在训练过程中出现 Vanishing/Exploding Gradients 的几率。即在每个隐藏层的激活函数之前或之后加一层处理，将输入值归一化和偏移。</p>\n<p>但 Batch normalization 会增加计算量，因此需要加速训练的话，还是先考虑 ELU+He 初始化权值是否有用。</p>\n<h2 id=\"使用训练好的神经层\"><a href=\"#使用训练好的神经层\" class=\"headerlink\" title=\"使用训练好的神经层\"></a>使用训练好的神经层</h2><p>可以寻找一个已经训练好的神经网络，并使用该网络的底层加速我们的训练，此方法也被称为<em>迁移学习</em>。</p>\n<p>首先冻结所有的神经网络层，在此网络上进行训练，并评估其表现。之后解冻最顶部（靠近输出）的一到两层，使用向后传播对其参数微调，评估网络性能是否有改进。使用的训练数据越多，可解冻的层数可以相应增加。注意在解冻层时适当减小学习率，这样可以最大程度避免对原始参数的扰动。</p>\n<p>如果使用了上述方法但没有明显效果，且可用于训练的数据较少，可以尝试去掉顶部的层，并重复上述方法训练，直到找到适合的网络结构为止。</p>\n<h2 id=\"Dropout\"><a href=\"#Dropout\" class=\"headerlink\" title=\"Dropout\"></a>Dropout</h2><p>在每一个训练步前，网络中所有的神经元（包含输入层，但往往不包含输出层）均有一定几率 <mjx-container class=\"MathJax\" jax=\"SVG\"><svg style=\"vertical-align: -0.439ex\" xmlns=\"http://www.w3.org/2000/svg\" width=\"1.138ex\" height=\"1.439ex\" role=\"img\" focusable=\"false\" viewBox=\"0 -442 503 636\"><g stroke=\"currentColor\" fill=\"currentColor\" stroke-width=\"0\" transform=\"scale(1,-1)\"><g data-mml-node=\"math\"><g data-mml-node=\"mi\"><path data-c=\"1D45D\" d=\"M23 287Q24 290 25 295T30 317T40 348T55 381T75 411T101 433T134 442Q209 442 230 378L240 387Q302 442 358 442Q423 442 460 395T497 281Q497 173 421 82T249 -10Q227 -10 210 -4Q199 1 187 11T168 28L161 36Q160 35 139 -51T118 -138Q118 -144 126 -145T163 -148H188Q194 -155 194 -157T191 -175Q188 -187 185 -190T172 -194Q170 -194 161 -194T127 -193T65 -192Q-5 -192 -24 -194H-32Q-39 -187 -39 -183Q-37 -156 -26 -148H-6Q28 -147 33 -136Q36 -130 94 103T155 350Q156 355 156 364Q156 405 131 405Q109 405 94 377T71 316T59 280Q57 278 43 278H29Q23 284 23 287ZM178 102Q200 26 252 26Q282 26 310 49T356 107Q374 141 392 215T411 325V331Q411 405 350 405Q339 405 328 402T306 393T286 380T269 365T254 350T243 336T235 326L232 322Q232 321 229 308T218 264T204 212Q178 106 178 102Z\"></path></g></g></g></svg></mjx-container> 暂时不参与此训练步（指被完全忽略，并非“冻结”），但在下一个训练步中有可能被再次激活。<mjx-container class=\"MathJax\" jax=\"SVG\"><svg style=\"vertical-align: -0.439ex\" xmlns=\"http://www.w3.org/2000/svg\" width=\"1.138ex\" height=\"1.439ex\" role=\"img\" focusable=\"false\" viewBox=\"0 -442 503 636\"><g stroke=\"currentColor\" fill=\"currentColor\" stroke-width=\"0\" transform=\"scale(1,-1)\"><g data-mml-node=\"math\"><g data-mml-node=\"mi\"><path data-c=\"1D45D\" d=\"M23 287Q24 290 25 295T30 317T40 348T55 381T75 411T101 433T134 442Q209 442 230 378L240 387Q302 442 358 442Q423 442 460 395T497 281Q497 173 421 82T249 -10Q227 -10 210 -4Q199 1 187 11T168 28L161 36Q160 35 139 -51T118 -138Q118 -144 126 -145T163 -148H188Q194 -155 194 -157T191 -175Q188 -187 185 -190T172 -194Q170 -194 161 -194T127 -193T65 -192Q-5 -192 -24 -194H-32Q-39 -187 -39 -183Q-37 -156 -26 -148H-6Q28 -147 33 -136Q36 -130 94 103T155 350Q156 355 156 364Q156 405 131 405Q109 405 94 377T71 316T59 280Q57 278 43 278H29Q23 284 23 287ZM178 102Q200 26 252 26Q282 26 310 49T356 107Q374 141 392 215T411 325V331Q411 405 350 405Q339 405 328 402T306 393T286 380T269 365T254 350T243 336T235 326L232 322Q232 321 229 308T218 264T204 212Q178 106 178 102Z\"></path></g></g></g></svg></mjx-container> 为超参数，称为 <em>dropout rate</em>。</p>\n<p>此时，每一次训练的网络结构都有所不同，因此最后的训练结果可视为这些网络的整体代表。</p>\n<p>一般 Dropout 仅参与到训练过程中，因此训练损失会有所高于验证损失，直接比较二者是有失妥当的。</p>\n","text":"Hands-on ml2 Ch.11 读书笔记。 Vanishing/Exploding Gradients在深度学习网络训练过程中，更深层次的网络梯度下降幅度越小，导致深层次的网络权重保持不变。或一些层梯度过大，每次权值更新的幅度很大（如 RNN）。导致不同层的学习速度相差较大...","link":"","photos":[],"count_time":{"symbolsCount":"1.8k","symbolsTime":"2 mins."},"categories":[{"name":"笔记","slug":"笔记","count":3,"path":"api/categories/笔记.json"}],"tags":[{"name":"读书笔记","slug":"读书笔记","count":3,"path":"api/tags/读书笔记.json"},{"name":"深度学习","slug":"深度学习","count":1,"path":"api/tags/深度学习.json"}],"toc":"<ol class=\"toc\"><li class=\"toc-item toc-level-2\"><a class=\"toc-link\" href=\"#Vanishing-Exploding-Gradients\"><span class=\"toc-text\">Vanishing&#x2F;Exploding Gradients</span></a></li><li class=\"toc-item toc-level-2\"><a class=\"toc-link\" href=\"#Glorot-and-He-Initialization\"><span class=\"toc-text\">Glorot and He Initialization</span></a></li><li class=\"toc-item toc-level-2\"><a class=\"toc-link\" href=\"#%E4%B8%8D%E6%94%B6%E6%95%9B%E7%9A%84%E6%BF%80%E6%B4%BB%E5%87%BD%E6%95%B0\"><span class=\"toc-text\">不收敛的激活函数</span></a></li><li class=\"toc-item toc-level-2\"><a class=\"toc-link\" href=\"#Batch-Normalization\"><span class=\"toc-text\">Batch Normalization</span></a></li><li class=\"toc-item toc-level-2\"><a class=\"toc-link\" href=\"#%E4%BD%BF%E7%94%A8%E8%AE%AD%E7%BB%83%E5%A5%BD%E7%9A%84%E7%A5%9E%E7%BB%8F%E5%B1%82\"><span class=\"toc-text\">使用训练好的神经层</span></a></li><li class=\"toc-item toc-level-2\"><a class=\"toc-link\" href=\"#Dropout\"><span class=\"toc-text\">Dropout</span></a></li></ol>","author":{"name":"Ryanhui","slug":"blog-author","avatar":"https://avatars1.githubusercontent.com/u/43368274?s=460&u=50a78f666213a52518d0076c1ca1cd9862076167&v=4","link":"/","description":"写博客，不过是孤芳自赏","socials":{"github":"https://github.com/Ranhuiryan","twitter":"","stackoverflow":"","wechat":"","qq":"","weibo":"https://weibo.com/wbgjh","zhihu":"","csdn":"","juejin":"","customs":{"bilibili":{"icon":"/svg/bilibili-fill.svg","link":"https://space.bilibili.com/5472735"},"researchgate":{"icon":"/svg/researchgate.svg","link":"https://www.researchgate.net/profile/Guo-Jihong"}}}},"mapped":true,"prev_post":{"title":"PVE 虚拟环境下 LEDE 软路由&黑群晖一体机搭建踩坑实录","uid":"071e075cf4eb65abed3b589cda67a89f","slug":"PVE虚拟环境LEDE软路由-黑群晖一体机搭建踩坑实录","date":"2020-02-21T11:57:07.000Z","updated":"2021-08-19T08:44:02.398Z","comments":true,"path":"api/articles/PVE虚拟环境LEDE软路由-黑群晖一体机搭建踩坑实录.json","keywords":null,"cover":"https://image-assets.mihuashi.com/2021/08/14/00/Fi7W8zObmWFkqpT-FOAoHgVXNwkl.jpeg","text":" 奇怪的运营商司马增加了 本文主要记录了在一台老 Intel 笔记本上安装 PVE 系统，部署 LEDE 软路由（主路由）以及黑群晖的瞎折腾过程。 整个网络结构的规划为：光纤 -&gt; 光猫（桥接）-&gt; 交换机（有线终端接入）-&gt; 无线 AP（无线终端接入）-&gt...","link":"","photos":[],"count_time":{"symbolsCount":"6.7k","symbolsTime":"6 mins."},"categories":[{"name":"技术","slug":"技术","count":2,"path":"api/categories/技术.json"}],"tags":[{"name":"网络技术","slug":"网络技术","count":2,"path":"api/tags/网络技术.json"}],"author":{"name":"Ryanhui","slug":"blog-author","avatar":"https://avatars1.githubusercontent.com/u/43368274?s=460&u=50a78f666213a52518d0076c1ca1cd9862076167&v=4","link":"/","description":"写博客，不过是孤芳自赏","socials":{"github":"https://github.com/Ranhuiryan","twitter":"","stackoverflow":"","wechat":"","qq":"","weibo":"https://weibo.com/wbgjh","zhihu":"","csdn":"","juejin":"","customs":{"bilibili":{"icon":"/svg/bilibili-fill.svg","link":"https://space.bilibili.com/5472735"},"researchgate":{"icon":"/svg/researchgate.svg","link":"https://www.researchgate.net/profile/Guo-Jihong"}}}}},"next_post":{"title":"HTML-动手学（7）","uid":"b7638511be4b1096798ab59fb8661e33","slug":"HTML-动手学（7）","date":"2019-10-16T11:12:59.000Z","updated":"2021-08-19T08:50:18.680Z","comments":true,"path":"api/articles/HTML-动手学（7）.json","keywords":null,"cover":"https://paradacreativa.es/wp-content/uploads/2019/12/C%C3%B3mo-abrir-archivos-HTML-foto.jpg","text":"Codecademy HTML 教程学习笔记 Forms（表单） How a Form Works（表单是如何工作的）使用 &lt;form&gt; 元素创建表单。 &lt;form action&#x3D;&quot;&#x2F;example.html&quot; metho...","link":"","photos":[],"count_time":{"symbolsCount":"1.5k","symbolsTime":"1 mins."},"categories":[{"name":"动手学","slug":"动手学","count":11,"path":"api/categories/动手学.json"},{"name":"HTML","slug":"动手学/HTML","count":7,"path":"api/categories/动手学/HTML.json"}],"tags":[{"name":"动手学","slug":"动手学","count":11,"path":"api/tags/动手学.json"},{"name":"HTML","slug":"HTML","count":7,"path":"api/tags/HTML.json"},{"name":"前端开发","slug":"前端开发","count":7,"path":"api/tags/前端开发.json"}],"author":{"name":"Ryanhui","slug":"blog-author","avatar":"https://avatars1.githubusercontent.com/u/43368274?s=460&u=50a78f666213a52518d0076c1ca1cd9862076167&v=4","link":"/","description":"写博客，不过是孤芳自赏","socials":{"github":"https://github.com/Ranhuiryan","twitter":"","stackoverflow":"","wechat":"","qq":"","weibo":"https://weibo.com/wbgjh","zhihu":"","csdn":"","juejin":"","customs":{"bilibili":{"icon":"/svg/bilibili-fill.svg","link":"https://space.bilibili.com/5472735"},"researchgate":{"icon":"/svg/researchgate.svg","link":"https://www.researchgate.net/profile/Guo-Jihong"}}}}}}