{"title":"训练深度学习网络的挑战和解决方法","uid":"a71e8b888458c8851815d0b96809bb00","slug":"训练深度学习网络的挑战和解决方法","date":"2019-12-27T09:46:48.000Z","updated":"2021-08-19T06:00:23.757Z","comments":true,"path":"api/articles/训练深度学习网络的挑战和解决方法.json","keywords":null,"cover":"https://image-assets.mihuashi.com/2021/08/15/09/FixrUuTczBtvQo8U9j-X2eCGMrzm.png","content":"<p>Hands-on ml2 Ch.11 读书笔记。</p>\n<span id=\"more\"></span>\n\n<h2 id=\"Vanishing-Exploding-Gradients\"><a href=\"#Vanishing-Exploding-Gradients\" class=\"headerlink\" title=\"Vanishing/Exploding Gradients\"></a>Vanishing/Exploding Gradients</h2><p>在深度学习网络训练过程中，更深层次的网络梯度下降幅度越小，导致深层次的网络权重保持不变。或一些层梯度过大，每次权值更新的幅度很大（如 RNN）。导致不同层的学习速度相差较大。</p>\n<p>这个问题的出现可归功于 Sigmoid 激活函数和正态分布权值初始化方法的使用。由于 Sigmoid 激活函数在正无穷和负无穷处收敛，其导数趋向于 0。</p>\n<h2 id=\"Glorot-and-He-Initialization\"><a href=\"#Glorot-and-He-Initialization\" class=\"headerlink\" title=\"Glorot and He Initialization\"></a>Glorot and He Initialization</h2><p>因此，建议在使用 Sigmoid 激活函数时采用 Glorot 初始化权值，或使用 ReLU 激活函数时采用 He 初始化权值，以及使用 SELU 激活函数时采用 LeCun 初始化权值。</p>\n<p>Keras 默认使用正态分布的 Glorot 初始化权值。</p>\n<h2 id=\"不收敛的激活函数\"><a href=\"#不收敛的激活函数\" class=\"headerlink\" title=\"不收敛的激活函数\"></a>不收敛的激活函数</h2><p>在深度学习网络中使用 ReLU 激活函数的效果比 Sigmoid 激活函数好很多。但会出现 <em>dying ReLUs</em> 的问题：一些神经元的激活值一直为0。</p>\n<p>可以使用 <em>leaky ReLU</em> 函数解决这个问题 $LeakyReLU_{\\alpha}(z)=max(\\alpha z, z)$。即引入一个超参数 $\\alpha$ 定义当输入值 $z&lt;0$ 时函数的“泄露量”。<em>Randomized leaky ReLU</em>(RReLU) 是 <em>leaky ReLU</em> 的变种，即在训练过程中随机指定某一范围内的 $\\alpha$ 值，测试时则取训练中 $\\alpha$ 的平均值。</p>\n<p>2015年，<em>exponential linear unit</em>（ELU）激活函数被提出。后来，有人提出在只由 dense 层构建的神经网络中使用 SELU 函数（S for scale），网络会 <em>self-normalize</em>：训练过程中，每一层的输出会保持平均值为0，标准差为1，即不会出现 Vanishing/Exploding Gradients。但出现这种现象需要一定条件：</p>\n<ul>\n<li>输入层必须是正则的（均值为0，标准差为1）；</li>\n<li>每个隐藏层的权值必须是 LeCun 初始化；</li>\n<li>网络的架构必须是连续的，例如在 RNN 中 SELU 的效果就不如其他激活函数；</li>\n<li>必须是只由 dense 层构建的神经网络，然而实际 SELU 在 CNN 中的效果也很好。</li>\n</ul>\n<p>因此选择激活函数：SELU&gt;ELU&gt;leaky ReLU（及其变种）&gt;ReLU&gt;tanh&gt;logistic(sigmoid)。如果网络不是 self-normalize 的，ELU 的效果会比 SELU 更佳。如果不考虑运行延迟，可以选用 leaky ReLU。不想调参可以用 Keras 使用的 $\\alpha$ 值（0.3）。算力允许的话，也可以使用交叉验证寻找其他激活函数，比如过拟合时考虑用 RReLU，训练集较大时用 PReLU。</p>\n<h2 id=\"Batch-Normalization\"><a href=\"#Batch-Normalization\" class=\"headerlink\" title=\"Batch Normalization\"></a>Batch Normalization</h2><p><em>Batch normalization</em> 可以降低网络在训练过程中出现 Vanishing/Exploding Gradients 的几率。即在每个隐藏层的激活函数之前或之后加一层处理，将输入值归一化和偏移。</p>\n<p>但 Batch normalization 会增加计算量，因此需要加速训练的话，还是先考虑 ELU+He 初始化权值是否有用。</p>\n<h2 id=\"使用训练好的神经层\"><a href=\"#使用训练好的神经层\" class=\"headerlink\" title=\"使用训练好的神经层\"></a>使用训练好的神经层</h2><p>可以寻找一个已经训练好的神经网络，并使用该网络的底层加速我们的训练，此方法也被称为<em>迁移学习</em>。</p>\n<p>首先冻结所有的神经网络层，在此网络上进行训练，并评估其表现。之后解冻最顶部（靠近输出）的一到两层，使用向后传播对其参数微调，评估网络性能是否有改进。使用的训练数据越多，可解冻的层数可以相应增加。注意在解冻层时适当减小学习率，这样可以最大程度避免对原始参数的扰动。</p>\n<p>如果使用了上述方法但没有明显效果，且可用于训练的数据较少，可以尝试去掉顶部的层，并重复上述方法训练，直到找到适合的网络结构为止。</p>\n<h2 id=\"Dropout\"><a href=\"#Dropout\" class=\"headerlink\" title=\"Dropout\"></a>Dropout</h2><p>在每一个训练步前，网络中所有的神经元（包含输入层，但往往不包含输出层）均有一定几率 $p$ 暂时不参与此训练步（指被完全忽略，并非“冻结”），但在下一个训练步中有可能被再次激活。$p$ 为超参数，称为 <em>dropout rate</em>。</p>\n<p>此时，每一次训练的网络结构都有所不同，因此最后的训练结果可视为这些网络的整体代表。</p>\n<p>一般 Dropout 仅参与到训练过程中，因此训练损失会有所高于验证损失，直接比较二者是有失妥当的。</p>\n","text":"Hands-on ml2 Ch.11 读书笔记。 Vanishing/Exploding Gradients在深度学习网络训练过程中，更深层次的网络梯度下降幅度越小，导致深层次的网络权重保持不变。或一些层梯度过大，每次权值更新的幅度很大（如 RNN）。导致不同层的学习速度相差较大...","link":"","photos":[],"count_time":{"symbolsCount":"1.9k","symbolsTime":"2 mins."},"categories":[{"name":"笔记","slug":"笔记","count":3,"path":"api/categories/笔记.json"}],"tags":[{"name":"读书笔记","slug":"读书笔记","count":3,"path":"api/tags/读书笔记.json"},{"name":"深度学习","slug":"深度学习","count":1,"path":"api/tags/深度学习.json"}],"toc":"<ol class=\"toc\"><li class=\"toc-item toc-level-2\"><a class=\"toc-link\" href=\"#Vanishing-Exploding-Gradients\"><span class=\"toc-text\">Vanishing&#x2F;Exploding Gradients</span></a></li><li class=\"toc-item toc-level-2\"><a class=\"toc-link\" href=\"#Glorot-and-He-Initialization\"><span class=\"toc-text\">Glorot and He Initialization</span></a></li><li class=\"toc-item toc-level-2\"><a class=\"toc-link\" href=\"#%E4%B8%8D%E6%94%B6%E6%95%9B%E7%9A%84%E6%BF%80%E6%B4%BB%E5%87%BD%E6%95%B0\"><span class=\"toc-text\">不收敛的激活函数</span></a></li><li class=\"toc-item toc-level-2\"><a class=\"toc-link\" href=\"#Batch-Normalization\"><span class=\"toc-text\">Batch Normalization</span></a></li><li class=\"toc-item toc-level-2\"><a class=\"toc-link\" href=\"#%E4%BD%BF%E7%94%A8%E8%AE%AD%E7%BB%83%E5%A5%BD%E7%9A%84%E7%A5%9E%E7%BB%8F%E5%B1%82\"><span class=\"toc-text\">使用训练好的神经层</span></a></li><li class=\"toc-item toc-level-2\"><a class=\"toc-link\" href=\"#Dropout\"><span class=\"toc-text\">Dropout</span></a></li></ol>","author":{"name":"Ryanhui","slug":"blog-author","avatar":"https://avatars1.githubusercontent.com/u/43368274?s=460&u=50a78f666213a52518d0076c1ca1cd9862076167&v=4","link":"/","description":"写博客，不过是孤芳自赏","socials":{"github":"https://weibo.com/wbgjh","twitter":"","stackoverflow":"","wechat":"","qq":"","weibo":"https://github.com/Ranhuiryan","zhihu":"","csdn":"","juejin":"","customs":{}}},"mapped":true,"prev_post":{"title":"PVE 虚拟环境下 LEDE 软路由&黑群晖一体机搭建踩坑实录","uid":"071e075cf4eb65abed3b589cda67a89f","slug":"PVE虚拟环境LEDE软路由-黑群晖一体机搭建踩坑实录","date":"2020-02-21T11:57:07.000Z","updated":"2021-08-19T06:18:57.964Z","comments":true,"path":"api/articles/PVE虚拟环境LEDE软路由-黑群晖一体机搭建踩坑实录.json","keywords":null,"cover":"https://image-assets.mihuashi.com/2021/08/14/00/Fi7W8zObmWFkqpT-FOAoHgVXNwkl.jpeg","text":" 蜜汁吐槽 奇怪的运营商司马增加了 本文主要记录了在一台老 Intel 笔记本上安装 PVE 系统，部署 LEDE 软路由（主路由）以及黑群晖的瞎折腾过程。 整个网络结构的规划为：光纤 -&gt; 光猫（桥接）-&gt; 交换机（有线终端接入）-&gt; 无线 AP（无线终端接入...","link":"","photos":[],"count_time":{"symbolsCount":"6.7k","symbolsTime":"6 mins."},"categories":[{"name":"技术","slug":"技术","count":2,"path":"api/categories/技术.json"}],"tags":[{"name":"网络技术","slug":"网络技术","count":2,"path":"api/tags/网络技术.json"}],"author":{"name":"Ryanhui","slug":"blog-author","avatar":"https://avatars1.githubusercontent.com/u/43368274?s=460&u=50a78f666213a52518d0076c1ca1cd9862076167&v=4","link":"/","description":"写博客，不过是孤芳自赏","socials":{"github":"https://weibo.com/wbgjh","twitter":"","stackoverflow":"","wechat":"","qq":"","weibo":"https://github.com/Ranhuiryan","zhihu":"","csdn":"","juejin":"","customs":{}}}},"next_post":{"title":"HTML-动手学（7）","uid":"b7638511be4b1096798ab59fb8661e33","slug":"HTML-动手学（7）","date":"2019-10-16T11:12:59.000Z","updated":"2021-08-19T06:09:25.866Z","comments":true,"path":"api/articles/HTML-动手学（7）.json","keywords":null,"cover":"https://paradacreativa.es/wp-content/uploads/2019/12/C%C3%B3mo-abrir-archivos-HTML-foto.jpg","text":"Codecademy HTML 教程学习笔记 Forms（表单） How a Form Works（表单是如何工作的）使用 &lt;form&gt; 元素创建表单。 &lt;form action&#x3D;&quot;&#x2F;example.html&quot; metho...","link":"","photos":[],"count_time":{"symbolsCount":"1.5k","symbolsTime":"1 mins."},"categories":[{"name":"动手学","slug":"动手学","count":11,"path":"api/categories/动手学.json"},{"name":"HTML","slug":"动手学/HTML","count":7,"path":"api/categories/动手学/HTML.json"}],"tags":[{"name":"动手学","slug":"动手学","count":11,"path":"api/tags/动手学.json"},{"name":"HTML","slug":"HTML","count":7,"path":"api/tags/HTML.json"},{"name":"前端开发","slug":"前端开发","count":7,"path":"api/tags/前端开发.json"}],"author":{"name":"Ryanhui","slug":"blog-author","avatar":"https://avatars1.githubusercontent.com/u/43368274?s=460&u=50a78f666213a52518d0076c1ca1cd9862076167&v=4","link":"/","description":"写博客，不过是孤芳自赏","socials":{"github":"https://weibo.com/wbgjh","twitter":"","stackoverflow":"","wechat":"","qq":"","weibo":"https://github.com/Ranhuiryan","zhihu":"","csdn":"","juejin":"","customs":{}}}}}